{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f079b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\MLM\\mlm\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\binit\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, BertForMaskedLM, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When I glance over my notes and records of the Sherlock Holmes cases between the years '82 and '90, I am faced by so many which present strange and interesting features that it is no easy matter to know which to choose and which to leave.\", 'Some, however, have already gained publicity through the papers, and others have not offered a field for those peculiar qualities which my friend possessed in so high a degree, and which it is the object of these papers to illustrate.', 'Some, too, have baffled his analytical skill, and would be, as narratives, beginnings without an ending, while others have been but partially cleared up, and have their explanations founded rather upon conjecture and surmise than on that absolute logical proof which was so dear to him.', 'There is, however, one of these last which was so remarkable in its details and so startling in its results that I am tempted to give some account of it in spite of the fact that there are points in connection with it which never have been, and probably never will be, entirely cleared up.', \"The year '87 furnished us with a long series of cases of greater or less interest, of which I retain the records.\", 'Among my headings under this one twelve months I find an account of the adventure of the Paradol Chamber, of the Amateur Mendicant Society, who held a luxurious club in the lower vault of a furniture warehouse, of the facts connected with the loss of the British barque \"Sophy Anderson\", of the singular adventures of the Grice Patersons in the island of Uffa, and finally of the Camberwell poisoning case.', \"In the latter, as may be remembered, Sherlock Holmes was able, by winding up the dead man's watch, to prove that it had been wound up two hours before, and that therefore the deceased had gone to bed within that time--a deduction which was of the greatest importance in clearing up the case.\", 'All these I may sketch out at some future date, but none of them present such singular features as the strange train of circumstances which I have now taken up my pen to describe.', 'It was in the latter days of September, and the equinoctial gales had set in with exceptional violence.', 'All day the wind had screamed and the rain had beaten against the windows, so that even here in the heart of great, hand-made London we were forced to raise our minds for the instant from the routine of life and to recognise the presence of those great elemental forces which shriek at mankind through the bars of his civilisation, like untamed beasts in a cage.', 'As evening drew in, the storm grew higher and louder, and the wind cried and sobbed like a child in the chimney.', \"Sherlock Holmes sat moodily at one side of the fireplace cross-indexing his records of crime, while I at the other was deep in one of Clark Russell's fine sea-stories until the howl of the gale from without seemed to blend with the text, and the splash of the rain to lengthen out into the long swash of the sea waves.\", \"My wife was on a visit to her mother's, and for a few days I was a dweller once more in my old quarters at Baker Street.\", '\"Why,\" said I, glancing up at my companion, \"that was surely the bell.', 'Who could come to-night? Some friend of yours, perhaps?\" \"Except yourself I have none,\" he answered.', '\"I do not encourage visitors.', '\" \"A client, then?\" \"If so, it is a serious case.', 'Nothing less would bring a man out on such a day and at such an hour.', \"But I take it that it is more likely to be some crony of the landlady's.\", '\" Sherlock Holmes was wrong in his conjecture, however, for there came a step in the passage and a tapping at the door.', 'He stretched out his long arm to turn the lamp away from himself and towards the vacant chair upon which a newcomer must sit.', '\"Come in!\" said he.', 'The man who entered was young, some two-and-twenty at the outside, well-groomed and trimly clad, with something of refinement and delicacy in his bearing.', 'The streaming umbrella which he held in his hand, and his long shining waterproof told of the fierce weather through which he had come.', 'He looked about him anxiously in the glare of the lamp, and I could see that his face was pale and his eyes heavy, like those of a man who is weighed down with some great anxiety.', '\"I owe you an apology,\" he said, raising his golden pince-nez to his eyes.', '\"I trust that I am not intruding.', 'I fear that I have brought some traces of the storm and rain into your snug chamber.', '\" \"Give me your coat and umbrella,\" said Holmes.', '\"They may rest here on the hook and will be dry presently.', 'You have come up from the south-west, I see.', '\" \"Yes, from Horsham.', '\" \"That clay and chalk mixture which I see upon your toe caps is quite distinctive.', '\" \"I have come for advice.', '\" \"That is easily got.', '\" \"And help.', '\" \"That is not always so easy.', '\" \"I have heard of you, Mr.', 'Holmes.', 'I heard from Major Prendergast how you saved him in the Tankerville Club scandal.', '\" \"Ah, of course.', 'He was wrongfully accused of cheating at cards.', '\" \"He said that you could solve anything.', '\" \"He said too much.', '\" \"That you are never beaten.', '\" \"I have been beaten four times--three times by men, and once by a woman.', '\" \"But what is that compared with the number of your successes?\" \"It is true that I have been generally successful.', '\" \"Then you may be so with me.', '\" \"I beg that you will draw your chair up to the fire and favour me with some details as to your case.', '\" \"It is no ordinary one.', '\" \"None of those which come to me are.', 'I am the last court of appeal.', '\" \"And yet I question, sir, whether, in all your experience, you have ever listened to a more mysterious and inexplicable chain of events than those which have happened in my own family.', '\" \"You fill me with interest,\" said Holmes.', '\"Pray give us the essential facts from the commencement, and I can afterwards question you as to those details which seem to me to be most important.', '\" The young man pulled his chair up and pushed his wet feet out towards the blaze.', '\"My name,\" said he, \"is John Openshaw, but my own affairs have, as far as I can understand, little to do with this awful business.', 'It is a hereditary matter; so in order to give you an idea of the facts, I must go back to the commencement of the affair.', '\"You must know that my grandfather had two sons--my uncle Elias and my father Joseph.', 'My father had a small factory at Coventry, which he enlarged at the time of the invention of bicycling.', 'He was a patentee of the Openshaw unbreakable tire, and his business met with such success that he was able to sell it and to retire upon a handsome competence.', '\"My uncle Elias emigrated to America when he was a young man and became a planter in Florida, where he was reported to have done very well.', \"At the time of the war he fought in Jackson's army, and afterwards under Hood, where he rose to be a colonel.\", 'When Lee laid down his arms my uncle returned to his plantation, where he remained for three or four years.', 'About 1869 or 1870 he came back to Europe and took a small estate in Sussex, near Horsham.', 'He had made a very considerable fortune in the States, and his reason for leaving them was his aversion to the negroes, and his dislike of the Republican policy in extending the franchise to them.', 'He was a singular man, fierce and quick-tempered, very foul-mouthed when he was angry, and of a most retiring disposition.', 'During all the years that he lived at Horsham, I doubt if ever he set foot in the town.', 'He had a garden and two or three fields round his house, and there he would take his exercise, though very often for weeks on end he would never leave his room.', 'He drank a great deal of brandy and smoked very heavily, but he would see no society and did not want any friends, not even his own brother.', '\"He didn\\'t mind me; in fact, he took a fancy to me, for at the time when he saw me first I was a youngster of twelve or so.', 'This would be in the year 1878, after he had been eight or nine years in England.', 'He begged my father to let me live with him and he was very kind to me in his way.', 'When he was sober he used to be fond of playing backgammon and draughts with me, and he would make me his representative both with the servants and with the tradespeople, so that by the time that I was sixteen I was quite master of the house.', 'I kept all the keys and could go where I liked and do what I liked, so long as I did not disturb him in his privacy.', 'There was one singular exception, however, for he had a single room, a lumber-room up among the attics, which was invariably locked, and which he would never permit either me or anyone else to enter.', \"With a boy's curiosity I have peeped through the keyhole, but I was never able to see more than such a collection of old trunks and bundles as would be expected in such a room.\", '\"One day--it was in March, 1883--a letter with a foreign stamp lay upon the table in front of the colonel\\'s plate.', 'It was not a common thing for him to receive letters, for his bills were all paid in ready money, and he had no friends of any sort.', \"'From India!' said he as he took it up, 'Pondicherry postmark! What can this be?' Opening it hurriedly, out there jumped five little dried orange pips, which pattered down upon his plate.\", 'I began to laugh at this, but the laugh was struck from my lips at the sight of his face.', \"His lip had fallen, his eyes were protruding, his skin the colour of putty, and he glared at the envelope which he still held in his trembling hand, 'K.\", 'K.', 'K.', '!\\' he shrieked, and then, \\'My God, my God, my sins have overtaken me!\\' \"\\'What is it, uncle?\\' I cried.', '\"\\'Death,\\' said he, and rising from the table he retired to his room, leaving me palpitating with horror.', 'I took up the envelope and saw scrawled in red ink upon the inner flap, just above the gum, the letter K three times repeated.', 'There was nothing else save the five dried pips.', 'What could be the reason of his overpowering terror? I left the breakfast-table, and as I ascended the stair I met him coming down with an old rusty key, which must have belonged to the attic, in one hand, and a small brass box, like a cashbox, in the other.', '\"\\'They may do what they like, but I\\'ll checkmate them still,\\' said he with an oath.', \"'Tell Mary that I shall want a fire in my room to-day, and send down to Fordham, the Horsham lawyer.\", '\\' \"I did as he ordered, and when the lawyer arrived I was asked to step up to the room.', 'The fire was burning brightly, and in the grate there was a mass of black, fluffy ashes, as of burned paper, while the brass box stood open and empty beside it.', 'As I glanced at the box I noticed, with a start, that upon the lid was printed the treble K which I had read in the morning upon the envelope.', '\"\\'I wish you, John,\\' said my uncle, \\'to witness my will.', 'I leave my estate, with all its advantages and all its disadvantages, to my brother, your father, whence it will, no doubt, descend to you.', 'If you can enjoy it in peace, well and good! If you find you cannot, take my advice, my boy, and leave it to your deadliest enemy.', \"I am sorry to give you such a two-edged thing, but I can't say what turn things are going to take.\", 'Kindly sign the paper where Mr.', 'Fordham shows you.', '\\' \"I signed the paper as directed, and the lawyer took it away with him.', 'The singular incident made, as you may think, the deepest impression upon me, and I pondered over it and turned it every way in my mind without being able to make anything of it.', 'Yet I could not shake off the vague feeling of dread which it left behind, though the sensation grew less keen as the weeks passed and nothing happened to disturb the usual routine of our lives.', 'I could see a change in my uncle, however.', 'He drank more than ever, and he was less inclined for any sort of society.', 'Most of his time he would spend in his room, with the door locked upon the inside, but sometimes he would emerge in a sort of drunken frenzy and would burst out of the house and tear about the garden with a revolver in his hand, screaming out that he was afraid of no man, and that he was not to be cooped up, like a sheep in a pen, by man or devil.', 'When these hot fits were over, however, he would rush tumultuously in at the door and lock and bar it behind him, like a man who can brazen it out no longer against the terror which lies at the roots of his soul.', 'At such times I have seen his face, even on a cold day, glisten with moisture, as though it were new raised from a basin.', '\"Well, to come to an end of the matter, Mr.', 'Holmes, and not to abuse your patience, there came a night when he made one of those drunken sallies from which he never came back.', 'We found him, when we went to search for him, face downward in a little green-scummed pool, which lay at the foot of the garden.', \"There was no sign of any violence, and the water was but two feet deep, so that the jury, having regard to his known eccentricity, brought in a verdict of 'suicide.\", \"' But I, who knew how he winced from the very thought of death, had much ado to persuade myself that he had gone out of his way to meet it.\", 'The matter passed, however, and my father entered into possession of the estate, and of some Â£14,000, which lay to his credit at the bank.', '\" \"One moment,\" Holmes interposed, \"your statement is, I foresee, one of the most remarkable to which I have ever listened.', 'Let me have the date of the reception by your uncle of the letter, and the date of his supposed suicide.', '\" \"The letter arrived on March 10, 1883.', 'His death was seven weeks later, upon the night of May 2nd.', '\" \"Thank you.', 'Pray proceed.', '\" \"When my father took over the Horsham property, he, at my request, made a careful examination of the attic, which had been always locked up.', 'We found the brass box there, although its contents had been destroyed.', 'On the inside of the cover was a paper label, with the initials of K.', 'K.', 'K.', \"repeated upon it, and 'Letters, memoranda, receipts, and a register' written beneath.\", 'These, we presume, indicated the nature of the papers which had been destroyed by Colonel Openshaw.', \"For the rest, there was nothing of much importance in the attic save a great many scattered papers and note-books bearing upon my uncle's life in America.\", 'Some of them were of the war time and showed that he had done his duty well and had borne the repute of a brave soldier.', 'Others were of a date during the reconstruction of the Southern states, and were mostly concerned with politics, for he had evidently taken a strong part in opposing the carpet-bag politicians who had been sent down from the North.', '\"Well, it was the beginning of \\'84 when my father came to live at Horsham, and all went as well as possible with us until the January of \\'85.', 'On the fourth day after the new year I heard my father give a sharp cry of surprise as we sat together at the breakfast-table.', 'There he was, sitting with a newly opened envelope in one hand and five dried orange pips in the outstretched palm of the other one.', 'He had always laughed at what he called my cock-and-bull story about the colonel, but he looked very scared and puzzled now that the same thing had come upon himself.', '\"\\'Why, what on earth does this mean, John?\\' he stammered.', '\"My heart had turned to lead.', \"'It is K.\", 'K.', 'K.', \",' said I.\", '\"He looked inside the envelope.', \"'So it is,' he cried.\", \"'Here are the very letters.\", 'But what is this written above them?\\' \"\\'Put the papers on the sundial,\\' I read, peeping over his shoulder.', '\"\\'What papers? What sundial?\\' he asked.', '\"\\'The sundial in the garden.', \"There is no other,' said I; 'but the papers must be those that are destroyed.\", '\\' \"\\'Pooh!\\' said he, gripping hard at his courage.', \"'We are in a civilised land here, and we can't have tomfoolery of this kind.\", 'Where does the thing come from?\\' \"\\'From Dundee,\\' I answered, glancing at the postmark.', '\"\\'Some preposterous practical joke,\\' said he.', \"'What have I to do with sundials and papers? I shall take no notice of such nonsense.\", '\\' \"\\'I should certainly speak to the police,\\' I said.', '\"\\'And be laughed at for my pains.', 'Nothing of the sort.', '\\' \"\\'Then let me do so?\\' \"\\'No, I forbid you.', \"I won't have a fuss made about such nonsense.\", '\\' \"It was in vain to argue with him, for he was a very obstinate man.', 'I went about, however, with a heart which was full of forebodings.', '\"On the third day after the coming of the letter my father went from home to visit an old friend of his, Major Freebody, who is in command of one of the forts upon Portsdown Hill.', 'I was glad that he should go, for it seemed to me that he was farther from danger when he was away from home.', 'In that, however, I was in error.', 'Upon the second day of his absence I received a telegram from the major, imploring me to come at once.', 'My father had fallen over one of the deep chalk-pits which abound in the neighbourhood, and was lying senseless, with a shattered skull.', 'I hurried to him, but he passed away without having ever recovered his consciousness.', \"He had, as it appears, been returning from Fareham in the twilight, and as the country was unknown to him, and the chalk-pit unfenced, the jury had no hesitation in bringing in a verdict of 'death from accidental causes.\", \"' Carefully as I examined every fact connected with his death, I was unable to find anything which could suggest the idea of murder.\", 'There were no signs of violence, no footmarks, no robbery, no record of strangers having been seen upon the roads.', 'And yet I need not tell you that my mind was far from at ease, and that I was well-nigh certain that some foul plot had been woven round him.', '\"In this sinister way I came into my inheritance.', \"You will ask me why I did not dispose of it? I answer, because I was well convinced that our troubles were in some way dependent upon an incident in my uncle's life, and that the danger would be as pressing in one house as in another.\", '\"It was in January, \\'85, that my poor father met his end, and two years and eight months have elapsed since then.', 'During that time I have lived happily at Horsham, and I had begun to hope that this curse had passed away from the family, and that it had ended with the last generation.', 'I had begun to take comfort too soon, however; yesterday morning the blow fell in the very shape in which it had come upon my father.', '\" The young man took from his waistcoat a crumpled envelope, and turning to the table he shook out upon it five little dried orange pips.', '\"This is the envelope,\" he continued.', '\"The postmark is London--eastern division.', \"Within are the very words which were upon my father's last message: 'K.\", 'K.', 'K.', \"'; and then 'Put the papers on the sundial.\", '\\'\" \"What have you done?\" asked Holmes.', '\"Nothing.', '\" \"Nothing?\" \"To tell the truth\"--he sank his face into his thin, white hands--\"I have felt helpless.', 'I have felt like one of those poor rabbits when the snake is writhing towards it.', 'I seem to be in the grasp of some resistless, inexorable evil, which no foresight and no precautions can guard against.', '\" \"Tut! tut!\" cried Sherlock Holmes.', '\"You must act, man, or you are lost.', 'Nothing but energy can save you.', 'This is no time for despair.', '\" \"I have seen the police.', '\" \"Ah!\" \"But they listened to my story with a smile.', 'I am convinced that the inspector has formed the opinion that the letters are all practical jokes, and that the deaths of my relations were really accidents, as the jury stated, and were not to be connected with the warnings.', '\" Holmes shook his clenched hands in the air.', '\"Incredible imbecility!\" he cried.', '\"They have, however, allowed me a policeman, who may remain in the house with me.', '\" \"Has he come with you to-night?\" \"No.', 'His orders were to stay in the house.', '\" Again Holmes raved in the air.', '\"Why did you come to me,\" he cried, \"and, above all, why did you not come at once?\" \"I did not know.', 'It was only to-day that I spoke to Major Prendergast about my troubles and was advised by him to come to you.', '\" \"It is really two days since you had the letter.', 'We should have acted before this.', 'You have no further evidence, I suppose, than that which you have placed before us--no suggestive detail which might help us?\" \"There is one thing,\" said John Openshaw.', 'He rummaged in his coat pocket, and, drawing out a piece of discoloured, blue-tinted paper, he laid it out upon the table.', '\"I have some remembrance,\" said he, \"that on the day when my uncle burned the papers I observed that the small, unburned margins which lay amid the ashes were of this particular colour.', 'I found this single sheet upon the floor of his room, and I am inclined to think that it may be one of the papers which has, perhaps, fluttered out from among the others, and in that way has escaped destruction.', 'Beyond the mention of pips, I do not see that it helps us much.', 'I think myself that it is a page from some private diary.', \"The writing is undoubtedly my uncle's.\", '\" Holmes moved the lamp, and we both bent over the sheet of paper, which showed by its ragged edge that it had indeed been torn from a book.', 'It was headed, \"March, 1869,\" and beneath were the following enigmatical notices: 4th.', 'Hudson came.', 'Same old platform.', '7th.', 'Set the pips on McCauley, Paramore, and John Swain, of St.', 'Augustine.', '9th.', 'McCauley cleared.', '10th.', 'John Swain cleared.', '12th.', 'Visited Paramore.', 'All well.', '\"Thank you!\" said Holmes, folding up the paper and returning it to our visitor.', '\"And now you must on no account lose another instant.', 'We cannot spare time even to discuss what you have told me.', 'You must get home instantly and act.', '\" \"What shall I do?\" \"There is but one thing to do.', 'It must be done at once.', 'You must put this piece of paper which you have shown us into the brass box which you have described.', 'You must also put in a note to say that all the other papers were burned by your uncle, and that this is the only one which remains.', 'You must assert that in such words as will carry conviction with them.', 'Having done this, you must at once put the box out upon the sundial, as directed.', 'Do you understand?\" \"Entirely.', '\" \"Do not think of revenge, or anything of the sort, at present.', 'I think that we may gain that by means of the law; but we have our web to weave, while theirs is already woven.', 'The first consideration is to remove the pressing danger which threatens you.', 'The second is to clear up the mystery and to punish the guilty parties.', '\" \"I thank you,\" said the young man, rising and pulling on his overcoat.', '\"You have given me fresh life and hope.', 'I shall certainly do as you advise.', '\" \"Do not lose an instant.', 'And, above all, take care of yourself in the meanwhile, for I do not think that there can be a doubt that you are threatened by a very real and imminent danger.', 'How do you go back?\" \"By train from Waterloo.', '\" \"It is not yet nine.', 'The streets will be crowded, so I trust that you may be in safety.', 'And yet you cannot guard yourself too closely.', '\" \"I am armed.', '\" \"That is well.', 'To-morrow I shall set to work upon your case.', '\" \"I shall see you at Horsham, then?\" \"No, your secret lies in London.', 'It is there that I shall seek it.', '\" \"Then I shall call upon you in a day, or in two days, with news as to the box and the papers.', 'I shall take your advice in every particular.', '\" He shook hands with us and took his leave.', 'Outside the wind still screamed and the rain splashed and pattered against the windows.', 'This strange, wild story seemed to have come to us from amid the mad elements--blown in upon us like a sheet of sea-weed in a gale--and now to have been reabsorbed by them once more.', 'Sherlock Holmes sat for some time in silence, with his head sunk forward and his eyes bent upon the red glow of the fire.', 'Then he lit his pipe, and leaning back in his chair he watched the blue smoke-rings as they chased each other up to the ceiling.', '\"I think, Watson,\" he remarked at last, \"that of all our cases we have had none more fantastic than this.', '\" \"Save, perhaps, the Sign of Four.', '\" \"Well, yes.', 'Save, perhaps, that.', 'And yet this John Openshaw seems to me to be walking amid even greater perils than did the Sholtos.', '\" \"But have you,\" I asked, \"formed any definite conception as to what these perils are?\" \"There can be no question as to their nature,\" he answered.', '\"Then what are they? Who is this K.', 'K.', 'K.', ', and why does he pursue this unhappy family?\" Sherlock Holmes closed his eyes and placed his elbows upon the arms of his chair, with his finger-tips together.', '\"The ideal reasoner,\" he remarked, \"would, when he had once been shown a single fact in all its bearings, deduce from it not only all the chain of events which led up to it but also all the results which would follow from it.', 'As Cuvier could correctly describe a whole animal by the contemplation of a single bone, so the observer who has thoroughly understood one link in a series of incidents should be able to accurately state all the other ones, both before and after.', 'We have not yet grasped the results which the reason alone can attain to.', 'Problems may be solved in the study which have baffled all those who have sought a solution by the aid of their senses.', 'To carry the art, however, to its highest pitch, it is necessary that the reasoner should be able to utilise all the facts which have come to his knowledge; and this in itself implies, as you will readily see, a possession of all knowledge, which, even in these days of free education and encyclopaedias, is a somewhat rare accomplishment.', 'It is not so impossible, however, that a man should possess all knowledge which is likely to be useful to him in his work, and this I have endeavoured in my case to do.', 'If I remember rightly, you on one occasion, in the early days of our friendship, defined my limits in a very precise fashion.', '\" \"Yes,\" I answered, laughing.', '\"It was a singular document.', 'Philosophy, astronomy, and politics were marked at zero, I remember.', 'Botany variable, geology profound as regards the mud-stains from any region within fifty miles of town, chemistry eccentric, anatomy unsystematic, sensational literature and crime records unique, violin-player, boxer, swordsman, lawyer, and self-poisoner by cocaine and tobacco.', 'Those, I think, were the main points of my analysis.', '\" Holmes grinned at the last item.', '\"Well,\" he said, \"I say now, as I said then, that a man should keep his little brain-attic stocked with all the furniture that he is likely to use, and the rest he can put away in the lumber-room of his library, where he can get it if he wants it.', 'Now, for such a case as the one which has been submitted to us to-night, we need certainly to muster all our resources.', \"Kindly hand me down the letter K of the 'American Encyclopaedia' which stands upon the shelf beside you.\", 'Thank you.', 'Now let us consider the situation and see what may be deduced from it.', 'In the first place, we may start with a strong presumption that Colonel Openshaw had some very strong reason for leaving America.', 'Men at his time of life do not change all their habits and exchange willingly the charming climate of Florida for the lonely life of an English provincial town.', 'His extreme love of solitude in England suggests the idea that he was in fear of someone or something, so we may assume as a working hypothesis that it was fear of someone or something which drove him from America.', 'As to what it was he feared, we can only deduce that by considering the formidable letters which were received by himself and his successors.', 'Did you remark the postmarks of those letters?\" \"The first was from Pondicherry, the second from Dundee, and the third from London.', '\" \"From East London.', 'What do you deduce from that?\" \"They are all seaports.', 'That the writer was on board of a ship.', '\" \"Excellent.', 'We have already a clue.', 'There can be no doubt that the probability--the strong probability--is that the writer was on board of a ship.', 'And now let us consider another point.', 'In the case of Pondicherry, seven weeks elapsed between the threat and its fulfilment, in Dundee it was only some three or four days.', 'Does that suggest anything?\" \"A greater distance to travel.', '\" \"But the letter had also a greater distance to come.', '\" \"Then I do not see the point.', '\" \"There is at least a presumption that the vessel in which the man or men are is a sailing-ship.', 'It looks as if they always send their singular warning or token before them when starting upon their mission.', 'You see how quickly the deed followed the sign when it came from Dundee.', 'If they had come from Pondicherry in a steamer they would have arrived almost as soon as their letter.', 'But, as a matter of fact, seven weeks elapsed.', 'I think that those seven weeks represented the difference between the mail-boat which brought the letter and the sailing vessel which brought the writer.', '\" \"It is possible.', '\" \"More than that.', 'It is probable.', 'And now you see the deadly urgency of this new case, and why I urged young Openshaw to caution.', 'The blow has always fallen at the end of the time which it would take the senders to travel the distance.', 'But this one comes from London, and therefore we cannot count upon delay.', '\" \"Good God!\" I cried.', '\"What can it mean, this relentless persecution?\" \"The papers which Openshaw carried are obviously of vital importance to the person or persons in the sailing-ship.', 'I think that it is quite clear that there must be more than one of them.', \"A single man could not have carried out two deaths in such a way as to deceive a coroner's jury.\", 'There must have been several in it, and they must have been men of resource and determination.', 'Their papers they mean to have, be the holder of them who it may.', 'In this way you see K.', 'K.', 'K.', 'ceases to be the initials of an individual and becomes the badge of a society.', '\" \"But of what society?\" \"Have you never--\" said Sherlock Holmes, bending forward and sinking his voice--\"have you never heard of the Ku Klux Klan?\" \"I never have.', '\" Holmes turned over the leaves of the book upon his knee.', '\"Here it is,\" said he presently: \"\\'Ku Klux Klan.', 'A name derived from the fanciful resemblance to the sound produced by cocking a rifle.', 'This terrible secret society was formed by some ex-Confederate soldiers in the Southern states after the Civil War, and it rapidly formed local branches in different parts of the country, notably in Tennessee, Louisiana, the Carolinas, Georgia, and Florida.', 'Its power was used for political purposes, principally for the terrorising of the negro voters and the murdering and driving from the country of those who were opposed to its views.', 'Its outrages were usually preceded by a warning sent to the marked man in some fantastic but generally recognised shape--a sprig of oak-leaves in some parts, melon seeds or orange pips in others.', 'On receiving this the victim might either openly abjure his former ways, or might fly from the country.', 'If he braved the matter out, death would unfailingly come upon him, and usually in some strange and unforeseen manner.', 'So perfect was the organisation of the society, and so systematic its methods, that there is hardly a case upon record where any man succeeded in braving it with impunity, or in which any of its outrages were traced home to the perpetrators.', 'For some years the organisation flourished in spite of the efforts of the United States government and of the better classes of the community in the South.', 'Eventually, in the year 1869, the movement rather suddenly collapsed, although there have been sporadic outbreaks of the same sort since that date.', '\\' \"You will observe,\" said Holmes, laying down the volume, \"that the sudden breaking up of the society was coincident with the disappearance of Openshaw from America with their papers.', 'It may well have been cause and effect.', 'It is no wonder that he and his family have some of the more implacable spirits upon their track.', 'You can understand that this register and diary may implicate some of the first men in the South, and that there may be many who will not sleep easy at night until it is recovered.', '\" \"Then the page we have seen--\" \"Is such as we might expect.', \"It ran, if I remember right, 'sent the pips to A, B, and C'--that is, sent the society's warning to them.\", 'Then there are successive entries that A and B cleared, or left the country, and finally that C was visited, with, I fear, a sinister result for C.', 'Well, I think, Doctor, that we may let some light into this dark place, and I believe that the only chance young Openshaw has in the meantime is to do what I have told him.', 'There is nothing more to be said or to be done to-night, so hand me over my violin and let us try to forget for half an hour the miserable weather and the still more miserable ways of our fellow-men.', '\" It had cleared in the morning, and the sun was shining with a subdued brightness through the dim veil which hangs over the great city.', 'Sherlock Holmes was already at breakfast when I came down.', '\"You will excuse me for not waiting for you,\" said he; \"I have, I foresee, a very busy day before me in looking into this case of young Openshaw\\'s.', '\" \"What steps will you take?\" I asked.', '\"It will very much depend upon the results of my first inquiries.', 'I may have to go down to Horsham, after all.', '\" \"You will not go there first?\" \"No, I shall commence with the City.', 'Just ring the bell and the maid will bring up your coffee.', '\" As I waited, I lifted the unopened newspaper from the table and glanced my eye over it.', 'It rested upon a heading which sent a chill to my heart.', '\"Holmes,\" I cried, \"you are too late.', '\" \"Ah!\" said he, laying down his cup, \"I feared as much.', 'How was it done?\" He spoke calmly, but I could see that he was deeply moved.', '\"My eye caught the name of Openshaw, and the heading \\'Tragedy Near Waterloo Bridge.', '\\' Here is the account: \"Between nine and ten last night Police-Constable Cook, of the H Division, on duty near Waterloo Bridge, heard a cry for help and a splash in the water.', 'The night, however, was extremely dark and stormy, so that, in spite of the help of several passers-by, it was quite impossible to effect a rescue.', 'The alarm, however, was given, and, by the aid of the water-police, the body was eventually recovered.', 'It proved to be that of a young gentleman whose name, as it appears from an envelope which was found in his pocket, was John Openshaw, and whose residence is near Horsham.', 'It is conjectured that he may have been hurrying down to catch the last train from Waterloo Station, and that in his haste and the extreme darkness he missed his path and walked over the edge of one of the small landing-places for river steamboats.', 'The body exhibited no traces of violence, and there can be no doubt that the deceased had been the victim of an unfortunate accident, which should have the effect of calling the attention of the authorities to the condition of the riverside landing-stages.', '\" We sat in silence for some minutes, Holmes more depressed and shaken than I had ever seen him.', '\"That hurts my pride, Watson,\" he said at last.', '\"It is a petty feeling, no doubt, but it hurts my pride.', 'It becomes a personal matter with me now, and, if God sends me health, I shall set my hand upon this gang.', 'That he should come to me for help, and that I should send him away to his death--!\" He sprang from his chair and paced about the room in uncontrollable agitation, with a flush upon his sallow cheeks and a nervous clasping and unclasping of his long thin hands.', '\"They must be cunning devils,\" he exclaimed at last.', '\"How could they have decoyed him down there? The Embankment is not on the direct line to the station.', 'The bridge, no doubt, was too crowded, even on such a night, for their purpose.', 'Well, Watson, we shall see who will win in the long run.', 'I am going out now!\" \"To the police?\" \"No; I shall be my own police.', 'When I have spun the web they may take the flies, but not before.', '\" All day I was engaged in my professional work, and it was late in the evening before I returned to Baker Street.', 'Sherlock Holmes had not come back yet.', \"It was nearly ten o'clock before he entered, looking pale and worn.\", 'He walked up to the sideboard, and tearing a piece from the loaf he devoured it voraciously, washing it down with a long draught of water.', '\"You are hungry,\" I remarked.', '\"Starving.', 'It had escaped my memory.', 'I have had nothing since breakfast.', '\" \"Nothing?\" \"Not a bite.', 'I had no time to think of it.', '\" \"And how have you succeeded?\" \"Well.', '\" \"You have a clue?\" \"I have them in the hollow of my hand.', 'Young Openshaw shall not long remain unavenged.', 'Why, Watson, let us put their own devilish trade-mark upon them.', 'It is well thought of!\" \"What do you mean?\" He took an orange from the cupboard, and tearing it to pieces he squeezed out the pips upon the table.', 'Of these he took five and thrust them into an envelope.', 'On the inside of the flap he wrote \"S.', 'H.', 'for J.', 'O.', '\" Then he sealed it and addressed it to \"Captain James Calhoun, Barque Lone Star, Savannah, Georgia.', '\" \"That will await him when he enters port,\" said he, chuckling.', '\"It may give him a sleepless night.', 'He will find it as sure a precursor of his fate as Openshaw did before him.', '\" \"And who is this Captain Calhoun?\" \"The leader of the gang.', 'I shall have the others, but he first.', '\" \"How did you trace it, then?\" He took a large sheet of paper from his pocket, all covered with dates and names.', '\"I have spent the whole day,\" said he, \"over Lloyd\\'s registers and files of the old papers, following the future career of every vessel which touched at Pondicherry in January and February in \\'83.', 'There were thirty-six ships of fair tonnage which were reported there during those months.', 'Of these, one, the Lone Star, instantly attracted my attention, since, although it was reported as having cleared from London, the name is that which is given to one of the states of the Union.', '\" \"Texas, I think.', '\" \"I was not and am not sure which; but I knew that the ship must have an American origin.', '\" \"What then?\" \"I searched the Dundee records, and when I found that the barque Lone Star was there in January, \\'85, my suspicion became a certainty.', 'I then inquired as to the vessels which lay at present in the port of London.', '\" \"Yes?\" \"The Lone Star had arrived here last week.', 'I went down to the Albert Dock and found that she had been taken down the river by the early tide this morning, homeward bound to Savannah.', 'I wired to Gravesend and learned that she had passed some time ago, and as the wind is easterly I have no doubt that she is now past the Goodwins and not very far from the Isle of Wight.', '\" \"What will you do, then?\" \"Oh, I have my hand upon him.', 'He and the two mates, are as I learn, the only native-born Americans in the ship.', 'The others are Finns and Germans.', 'I know, also, that they were all three away from the ship last night.', 'I had it from the stevedore who has been loading their cargo.', 'By the time that their sailing-ship reaches Savannah the mail-boat will have carried this letter, and the cable will have informed the police of Savannah that these three gentlemen are badly wanted here upon a charge of murder.', '\" There is ever a flaw, however, in the best laid of human plans, and the murderers of John Openshaw were never to receive the orange pips which would show them that another, as cunning and as resolute as themselves, was upon their track.', 'Very long and very severe were the equinoctial gales that year.', 'We waited long for news of the Lone Star of Savannah, but none ever reached us.', 'We did at last hear that somewhere far out in the Atlantic a shattered stern-post of a boat was seen swinging in the trough of a wave, with the letters \"L.', 'S.', '\" carved upon it, and that is all which we shall ever know of the fate of the Lone Star.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleaning_func(text):\n",
    "    text = re.sub(r'[\\t\\n]', ' ', text)\n",
    "    text = re.sub(r'\\.(\\w)', r'. \\1', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.split(r'\\.\\s*', text)\n",
    "    \n",
    "    return [s.strip()+'.' for s in text if s.strip()]\n",
    "\n",
    "with open('five.txt', 'r') as text_file:\n",
    "    data = cleaning_func(text_file.read())\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0441c100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I glance over my notes and records of the Sherlock Holmes cases [MASK] the years '82 and '90, I am faced by so [MASK] which present strange and interesting features that it is no [MASK] matter to know which to choose and which to leave. Some, however, [MASK] already gained publicity [MASK] the papers, and others [MASK] not offered a field for those peculiar qualities which my friend possessed in so high a degree, and which it is the object of these papers to illustrate. Some, too, [MASK] baffled his analytical skill, and would be, as narratives, beginnings without an ending, while others [MASK] been but partially cleared up, and [MASK] their explanations founded rather upon conjecture and surmise than on that absolute logical proof which was so dear to him.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_data = ' '.join(data[:3])\n",
    "masked_sample_data = sample_data.replace(\"between\", '[MASK]').replace(\"many\", '[MASK]').replace(\"easy\", '[MASK]').replace(\"through\", '[MASK]').replace(\"have\", '[MASK]')\n",
    "masked_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e542ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = tokenizer(masked_sample_data, return_tensors='pt')\n",
    "type(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88b84c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[  101,  2043,  1045,  6054,  2058,  2026,  3964,  1998,  2636,  1997,\n",
       "          1996, 20052,  9106,  3572,   103,  1996,  2086,  1005,  6445,  1998,\n",
       "          1005,  3938,  1010,  1045,  2572,  4320,  2011,  2061,   103,  2029,\n",
       "          2556,  4326,  1998,  5875,  2838,  2008,  2009,  2003,  2053,   103,\n",
       "          3043,  2000,  2113,  2029,  2000,  5454,  1998,  2029,  2000,  2681,\n",
       "          1012,  2070,  1010,  2174,  1010,   103,  2525,  4227, 11845,   103,\n",
       "          1996,  4981,  1010,  1998,  2500,   103,  2025,  3253,  1037,  2492,\n",
       "          2005,  2216, 14099, 11647,  2029,  2026,  2767,  8679,  1999,  2061,\n",
       "          2152,  1037,  3014,  1010,  1998,  2029,  2009,  2003,  1996,  4874,\n",
       "          1997,  2122,  4981,  2000, 19141,  1012,  2070,  1010,  2205,  1010,\n",
       "           103, 29088,  2010, 17826,  8066,  1010,  1998,  2052,  2022,  1010,\n",
       "          2004, 22143,  1010, 16508,  2302,  2019,  4566,  1010,  2096,  2500,\n",
       "           103,  2042,  2021,  6822,  5985,  2039,  1010,  1998,   103,  2037,\n",
       "         17959,  2631,  2738,  2588, 22647,  1998,  7505, 28732,  2084,  2006,\n",
       "          2008,  7619, 11177,  6947,  2029,  2001,  2061,  6203,  2000,  2032,\n",
       "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fd56ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_token_value = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3af78514",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(data, return_tensors='pt', max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebcc6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input['labels'] = input.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a15f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([427, 512]), torch.Size([427, 512]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating masking logic\n",
    "\n",
    "rand = torch.rand(input.input_ids.shape)\n",
    "rand.shape, input.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3b245ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5935, 0.7336, 0.6031,  ..., 0.8877, 0.3073, 0.7764],\n",
       "        [0.3996, 0.4269, 0.4135,  ..., 0.2101, 0.6893, 0.6130],\n",
       "        [0.3558, 0.6497, 0.9041,  ..., 0.2630, 0.1745, 0.6749],\n",
       "        ...,\n",
       "        [0.3562, 0.6681, 0.0383,  ..., 0.8598, 0.6547, 0.0205],\n",
       "        [0.2237, 0.4863, 0.1467,  ..., 0.7530, 0.8038, 0.3825],\n",
       "        [0.9586, 0.5135, 0.8666,  ..., 0.7155, 0.5288, 0.6403]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dae64bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens = (rand<0.15)*(input.input_ids != 101)*(input.input_ids != 102)*(input.input_ids != 0)\n",
    "masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be6b5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens[1].nonzero()\n",
    "\n",
    "sentences = []\n",
    "for sent in range(masked_tokens.shape[0]):\n",
    "    sentences.append(torch.flatten(masked_tokens[sent].nonzero()).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984837f",
   "metadata": {},
   "source": [
    "### Masking values from the specified index in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "917b5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(masked_tokens.shape[0]):\n",
    "    input.input_ids[i, sentences[i]] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba5859ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71417ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab111df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Myowndataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings) -> None:\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c3a6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Myowndataset(input)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805818b1",
   "metadata": {},
   "source": [
    "## Training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4e4e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]C:\\Users\\binit\\AppData\\Local\\Temp\\ipykernel_16140\\3784079123.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  0%|          | 0/27 [01:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[32m     12\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     loss = output.loss\n\u001b[32m     15\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1281\u001b[39m, in \u001b[36mBertForMaskedLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1272\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1273\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[32m   1275\u001b[39m \u001b[33;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[32m   1276\u001b[39m \u001b[33;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1279\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1293\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1295\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1296\u001b[39m prediction_scores = \u001b[38;5;28mself\u001b[39m.cls(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    994\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1009\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:651\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    649\u001b[39m past_key_value = past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:553\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    542\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    543\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    550\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    552\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    562\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:492\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    475\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    482\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    483\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    484\u001b[39m         hidden_states,\n\u001b[32m    485\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         output_attentions,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435\u001b[39m, in \u001b[36mBertSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m    434\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\MLM\\mlm\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "epochs = 4\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"epochs: {epoch}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45b000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
